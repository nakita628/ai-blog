---
date: 2025-09-30
title: Python機械学習入門完全ガイド：初心者でもわかる実践的な例と解説
description: Pythonを使った機械学習の基本から実践的な例まで、初心者向けに分かりやすく解説。scikit-learn、pandas、numpyを使ったデータ分析、予測モデルの構築、評価方法など、機械学習の核心を実際のコード例と共に詳しく説明する。
tags:
  - python
  - machine-learning
  - scikit-learn
  - pandas
  - numpy
  - data-science
  - beginner
  - tutorial
prev:
  text: 'Faker.js入門完全ガイド：テストデータ生成の定番ライブラリを初心者向けに解説'
  link: '/posts/2025/09/29'
next:
  text: 'Rust入門完全ガイド：安全で高速なシステムプログラミング言語を初心者向けに解説'
  link: '/posts/2025/10/01'
---

# Python機械学習入門完全ガイド：初心者でもわかる実践的な例と解説

Pythonは機械学習分野で最も人気のあるプログラミング言語の一つです。豊富なライブラリと直感的な構文により、初心者でも機械学習を始めやすくなっています。この記事では、Pythonを使った機械学習の基本から実践的な例まで、初心者にも分かりやすく解説します。

## 機械学習とは何か

機械学習（Machine Learning）は、コンピュータがデータから自動的にパターンを学習し、新しいデータに対して予測や判断を行う技術です。主に以下の3つのタイプに分類されます：

- **教師あり学習**: 正解データを使って学習し、新しいデータを分類・予測
- **教師なし学習**: 正解データなしでデータの構造やパターンを発見
- **強化学習**: 環境との相互作用を通じて最適な行動を学習

## 必要なライブラリのインストール

### 基本的なライブラリのインストール

```bash
# pipを使用したインストール
pip install numpy pandas matplotlib seaborn scikit-learn jupyter

# condaを使用したインストール
conda install numpy pandas matplotlib seaborn scikit-learn jupyter

# 個別インストール
pip install numpy          # 数値計算
pip install pandas         # データ分析
pip install matplotlib     # グラフ描画
pip install seaborn        # 統計的グラフ
pip install scikit-learn   # 機械学習
pip install jupyter        # ノートブック環境
```

### ライブラリのインポート

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# グラフの設定
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
```

## データの準備と前処理

### サンプルデータの作成

```python
# サンプルデータの生成
np.random.seed(42)

# 住宅価格予測用のデータ
n_samples = 1000
data = {
    'area': np.random.normal(100, 30, n_samples),  # 面積（平方メートル）
    'rooms': np.random.randint(1, 6, n_samples),   # 部屋数
    'age': np.random.randint(0, 50, n_samples),    # 築年数
    'distance_to_center': np.random.normal(10, 5, n_samples),  # 都心からの距離
    'floor': np.random.randint(1, 21, n_samples),  # 階数
}

# 価格を他の変数から計算（現実的な関係性を模擬）
price = (
    data['area'] * 50 +  # 面積 × 単価
    data['rooms'] * 10000 +  # 部屋数による価値
    -data['age'] * 1000 +  # 築年数による減価
    -data['distance_to_center'] * 500 +  # 距離による減価
    data['floor'] * 2000 +  # 階数による価値
    np.random.normal(0, 10000, n_samples)  # ノイズ
)

data['price'] = price

# DataFrameに変換
df = pd.DataFrame(data)

print("データの基本情報:")
print(df.head())
print(f"\nデータの形状: {df.shape}")
print(f"\n基本統計:")
print(df.describe())
```

### データの可視化

```python
# データの可視化
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('住宅データの分布', fontsize=16)

# 各変数の分布
variables = ['area', 'rooms', 'age', 'distance_to_center', 'floor', 'price']
for i, var in enumerate(variables):
    row = i // 3
    col = i % 3
    axes[row, col].hist(df[var], bins=30, alpha=0.7, edgecolor='black')
    axes[row, col].set_title(f'{var}の分布')
    axes[row, col].set_xlabel(var)
    axes[row, col].set_ylabel('頻度')

plt.tight_layout()
plt.show()

# 相関行列の可視化
plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('変数間の相関行列')
plt.show()
```

## 教師あり学習：回帰問題

### 線形回帰による住宅価格予測

```python
# 特徴量とターゲットの分離
X = df[['area', 'rooms', 'age', 'distance_to_center', 'floor']]
y = df['price']

# 訓練データとテストデータの分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# データの標準化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 線形回帰モデルの訓練
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)

# 予測
y_pred = lr_model.predict(X_test_scaled)

# モデルの評価
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("線形回帰モデルの評価結果:")
print(f"平均二乗誤差 (MSE): {mse:.2f}")
print(f"平方根平均二乗誤差 (RMSE): {rmse:.2f}")
print(f"平均絶対誤差 (MAE): {mae:.2f}")
print(f"決定係数 (R²): {r2:.4f}")

# 特徴量の重要度
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'coefficient': lr_model.coef_
}).sort_values('coefficient', key=abs, ascending=False)

print("\n特徴量の重要度（係数の絶対値）:")
print(feature_importance)
```

### 予測結果の可視化

```python
# 予測結果の可視化
plt.figure(figsize=(12, 5))

# 実際の価格 vs 予測価格
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('実際の価格')
plt.ylabel('予測価格')
plt.title('実際の価格 vs 予測価格')
plt.grid(True, alpha=0.3)

# 残差プロット
plt.subplot(1, 2, 2)
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('予測価格')
plt.ylabel('残差')
plt.title('残差プロット')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## 教師あり学習：分類問題

### 顧客満足度予測の例

```python
# 顧客満足度データの生成
np.random.seed(42)
n_customers = 1000

customer_data = {
    'age': np.random.randint(18, 80, n_customers),
    'income': np.random.normal(50000, 20000, n_customers),
    'spending': np.random.normal(2000, 800, n_customers),
    'visits': np.random.poisson(5, n_customers),
    'complaints': np.random.poisson(0.5, n_customers),
    'support_calls': np.random.poisson(2, n_customers)
}

# 満足度を他の変数から計算
satisfaction_score = (
    customer_data['income'] * 0.00001 +
    customer_data['spending'] * 0.001 +
    customer_data['visits'] * 0.1 +
    -customer_data['complaints'] * 0.5 +
    -customer_data['support_calls'] * 0.2 +
    np.random.normal(0, 0.5, n_customers)
)

# 満足度を3段階に分類
customer_data['satisfaction'] = pd.cut(
    satisfaction_score,
    bins=3,
    labels=['低', '中', '高']
)

df_customer = pd.DataFrame(customer_data)

print("顧客データの基本情報:")
print(df_customer.head())
print(f"\n満足度の分布:")
print(df_customer['satisfaction'].value_counts())
```

### ロジスティック回帰による分類

```python
# 特徴量とターゲットの準備
X_customer = df_customer[['age', 'income', 'spending', 'visits', 'complaints', 'support_calls']]
y_customer = df_customer['satisfaction']

# データの分割
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
    X_customer, y_customer, test_size=0.2, random_state=42, stratify=y_customer
)

# データの標準化
scaler_c = StandardScaler()
X_train_c_scaled = scaler_c.fit_transform(X_train_c)
X_test_c_scaled = scaler_c.transform(X_test_c)

# ロジスティック回帰モデルの訓練
lr_classifier = LogisticRegression(random_state=42, max_iter=1000)
lr_classifier.fit(X_train_c_scaled, y_train_c)

# 予測
y_pred_c = lr_classifier.predict(X_test_c_scaled)

# モデルの評価
accuracy = accuracy_score(y_test_c, y_pred_c)
print(f"ロジスティック回帰の精度: {accuracy:.4f}")

print("\n分類レポート:")
print(classification_report(y_test_c, y_pred_c))

# 混同行列の可視化
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test_c, y_pred_c)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=lr_classifier.classes_,
            yticklabels=lr_classifier.classes_)
plt.title('混同行列')
plt.xlabel('予測値')
plt.ylabel('実際の値')
plt.show()
```

### ランダムフォレストによる分類

```python
# ランダムフォレストモデルの訓練
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_c_scaled, y_train_c)

# 予測
y_pred_rf = rf_classifier.predict(X_test_c_scaled)

# モデルの評価
accuracy_rf = accuracy_score(y_test_c, y_pred_rf)
print(f"ランダムフォレストの精度: {accuracy_rf:.4f}")

print("\nランダムフォレストの分類レポート:")
print(classification_report(y_test_c, y_pred_rf))

# 特徴量の重要度
feature_importance_rf = pd.DataFrame({
    'feature': X_customer.columns,
    'importance': rf_classifier.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(data=feature_importance_rf, x='importance', y='feature')
plt.title('ランダムフォレストの特徴量重要度')
plt.xlabel('重要度')
plt.tight_layout()
plt.show()
```

## 教師なし学習：クラスタリング

### K-meansクラスタリング

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# 顧客データのクラスタリング
X_cluster = df_customer[['age', 'income', 'spending', 'visits']]

# データの標準化
scaler_cluster = StandardScaler()
X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)

# 最適なクラスター数の決定（エルボー法）
inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_cluster_scaled)
    inertias.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('クラスター数')
plt.ylabel('Inertia')
plt.title('エルボー法による最適クラスター数の決定')
plt.grid(True, alpha=0.3)
plt.show()

# 最適なクラスター数でクラスタリング
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_cluster_scaled)

# 結果の可視化
df_cluster = df_customer.copy()
df_cluster['cluster'] = clusters

plt.figure(figsize=(15, 5))

# 収入 vs 支出
plt.subplot(1, 3, 1)
for i in range(optimal_k):
    cluster_data = df_cluster[df_cluster['cluster'] == i]
    plt.scatter(cluster_data['income'], cluster_data['spending'],
               label=f'クラスター {i}', alpha=0.7)
plt.xlabel('収入')
plt.ylabel('支出')
plt.title('収入 vs 支出')
plt.legend()

# 年齢 vs 訪問回数
plt.subplot(1, 3, 2)
for i in range(optimal_k):
    cluster_data = df_cluster[df_cluster['cluster'] == i]
    plt.scatter(cluster_data['age'], cluster_data['visits'],
               label=f'クラスター {i}', alpha=0.7)
plt.xlabel('年齢')
plt.ylabel('訪問回数')
plt.title('年齢 vs 訪問回数')
plt.legend()

# クラスターの分布
plt.subplot(1, 3, 3)
cluster_counts = df_cluster['cluster'].value_counts().sort_index()
plt.bar(cluster_counts.index, cluster_counts.values)
plt.xlabel('クラスター')
plt.ylabel('顧客数')
plt.title('各クラスターの顧客数')
plt.xticks(range(optimal_k))

plt.tight_layout()
plt.show()

# クラスターの特徴分析
print("各クラスターの特徴:")
for i in range(optimal_k):
    cluster_data = df_cluster[df_cluster['cluster'] == i]
    print(f"\nクラスター {i} ({len(cluster_data)}人):")
    print(f"  平均年齢: {cluster_data['age'].mean():.1f}歳")
    print(f"  平均収入: ${cluster_data['income'].mean():,.0f}")
    print(f"  平均支出: ${cluster_data['spending'].mean():,.0f}")
    print(f"  平均訪問回数: {cluster_data['visits'].mean():.1f}回")
```

## 実践的な例：住宅価格予測システム

### 完全な機械学習パイプライン

```python
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.svm import SVR

# 住宅価格予測のための完全なパイプライン
def create_ml_pipeline():
    """機械学習パイプラインの作成"""

    # データの準備
    X = df[['area', 'rooms', 'age', 'distance_to_center', 'floor']]
    y = df['price']

    # データの分割
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 複数のモデルを比較
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(alpha=1.0),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVR': SVR(kernel='rbf', C=1.0, gamma='scale')
    }

    results = {}

    for name, model in models.items():
        # パイプラインの作成
        pipeline = Pipeline([
            ('scaler', StandardScaler()),
            ('model', model)
        ])

        # クロスバリデーション
        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')

        # モデルの訓練
        pipeline.fit(X_train, y_train)

        # 予測
        y_pred = pipeline.predict(X_test)

        # 評価
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))

        results[name] = {
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'test_r2': r2,
            'test_rmse': rmse,
            'model': pipeline
        }

        print(f"{name}:")
        print(f"  CV R²: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})")
        print(f"  Test R²: {r2:.4f}")
        print(f"  Test RMSE: {rmse:.2f}")
        print()

    return results, X_test, y_test

# パイプラインの実行
results, X_test, y_test = create_ml_pipeline()
```

### ハイパーパラメータの最適化

```python
# ランダムフォレストのハイパーパラメータ最適化
def optimize_hyperparameters():
    """ハイパーパラメータの最適化"""

    X = df[['area', 'rooms', 'age', 'distance_to_center', 'floor']]
    y = df['price']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # パイプラインの作成
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', RandomForestRegressor(random_state=42))
    ])

    # ハイパーパラメータのグリッド
    param_grid = {
        'model__n_estimators': [50, 100, 200],
        'model__max_depth': [None, 10, 20, 30],
        'model__min_samples_split': [2, 5, 10],
        'model__min_samples_leaf': [1, 2, 4]
    }

    # グリッドサーチ
    grid_search = GridSearchCV(
        pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1
    )

    grid_search.fit(X_train, y_train)

    print("最適なハイパーパラメータ:")
    print(grid_search.best_params_)
    print(f"最適なスコア: {grid_search.best_score_:.4f}")

    # 最適なモデルで予測
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"テストセットでのR²: {r2:.4f}")
    print(f"テストセットでのRMSE: {rmse:.2f}")

    return best_model

# ハイパーパラメータ最適化の実行
best_model = optimize_hyperparameters()
```

### モデルの解釈と可視化

```python
# 最適なモデルの特徴量重要度
def analyze_model_importance(model):
    """モデルの特徴量重要度の分析"""

    # 特徴量名
    feature_names = ['area', 'rooms', 'age', 'distance_to_center', 'floor']

    # 特徴量重要度の取得
    importance = model.named_steps['model'].feature_importances_

    # 重要度の可視化
    plt.figure(figsize=(10, 6))
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=True)

    plt.barh(importance_df['feature'], importance_df['importance'])
    plt.xlabel('重要度')
    plt.title('特徴量の重要度')
    plt.grid(True, alpha=0.3)
    plt.show()

    return importance_df

# モデルの分析
importance_df = analyze_model_importance(best_model)
print("特徴量の重要度:")
print(importance_df)
```

## モデルの保存と読み込み

### モデルの保存

```python
import joblib
import pickle

# モデルの保存
def save_model(model, filename):
    """モデルの保存"""
    joblib.dump(model, f'{filename}.joblib')
    print(f"モデルを {filename}.joblib に保存しました")

# モデルの読み込み
def load_model(filename):
    """モデルの読み込み"""
    model = joblib.load(f'{filename}.joblib')
    print(f"モデルを {filename}.joblib から読み込みました")
    return model

# モデルの保存
save_model(best_model, 'house_price_model')

# モデルの読み込み（例）
# loaded_model = load_model('house_price_model')
```

### 予測関数の作成

```python
def predict_house_price(area, rooms, age, distance_to_center, floor):
    """住宅価格の予測関数"""

    # 入力データの準備
    input_data = pd.DataFrame({
        'area': [area],
        'rooms': [rooms],
        'age': [age],
        'distance_to_center': [distance_to_center],
        'floor': [floor]
    })

    # 予測
    prediction = best_model.predict(input_data)[0]

    return prediction

# 予測の例
sample_prediction = predict_house_price(
    area=120,      # 120平方メートル
    rooms=3,       # 3部屋
    age=10,        # 築10年
    distance_to_center=5,  # 都心から5km
    floor=5        # 5階
)

print(f"予測される住宅価格: ${sample_prediction:,.2f}")
```

## まとめ

Pythonを使った機械学習は、豊富なライブラリと直感的な構文により、初心者でも始めやすい技術です。主なポイントは以下の通りです：

- **データの前処理**: 欠損値の処理、標準化、特徴量エンジニアリング
- **モデルの選択**: 問題の種類（回帰・分類・クラスタリング）に応じた適切なアルゴリズムの選択
- **モデルの評価**: 適切な評価指標による性能の測定
- **ハイパーパラメータの最適化**: グリッドサーチやランダムサーチによる最適な設定の探索
- **モデルの解釈**: 特徴量重要度によるモデルの理解
- **モデルの保存と活用**: 実用的な予測システムの構築

機械学習を学ぶことで、データから価値のある洞察を引き出し、現実の問題を解決する能力を身につけることができます。

## 次のステップ

- より高度なアルゴリズムの学習（XGBoost、LightGBM等）
- 深層学習（TensorFlow、PyTorch）
- 自然言語処理（NLTK、spaCy）
- 画像認識（OpenCV、PIL）
- 時系列分析（ARIMA、LSTM）
- 機械学習の本番環境へのデプロイ
