---
date: 2025-08-31
title: TypeScriptで学ぶWebスクレイピング：実装からベストプラクティスまで完全ガイド
description: TypeScriptを使ったWebスクレイピングの実装方法を解説。Puppeteer、Cheerio、Playwrightを使った実践的な例と、法的・倫理的考慮事項、パフォーマンス最適化まで網羅。
tags:
    - typescript
    - web-scraping
    - puppeteer
    - cheerio
    - playwright
    - automation
    - data-extraction
    - web-development
prev:
    text: "TypeScriptで学ぶ現実世界のアルゴリズム：交通、経済、自然現象の数値モデル化"
    link: "/posts/2025/08/30"
next: false
---

# TypeScriptで学ぶWebスクレイピング：実装からベストプラクティスまで完全ガイド

Webスクレイピングは、Webサイトからデータを自動的に収集する技術です。この記事では、TypeScriptを使ったスクレイピングの実装方法、ベストプラクティス、法的・倫理的考慮事項まで包括的に解説します。

## スクレイピングとは何か

Webスクレイピングは、Webサイトから構造化されたデータを自動的に抽出するプロセスです。価格比較、ニュース収集、市場調査、SEO分析など、様々な用途で活用されています。

### スクレイピングの種類

- **静的スクレイピング**: HTMLから直接データを抽出
- **動的スクレイピング**: JavaScriptで生成されるコンテンツを取得
- **APIスクレイピング**: 公開APIからデータを取得
- **ヘッドレスブラウザ**: ブラウザエンジンを使用したスクレイピング

## 基本的なスクレイピング実装

### 1. Cheerioを使った静的スクレイピング

Cheerioは、サーバーサイドでjQueryライクなAPIを提供するライブラリです。

```typescript
import * as cheerio from 'cheerio';
import axios from 'axios';

type NewsArticle = {
  readonly title: string;
  readonly url: string;
  readonly summary: string;
  readonly publishedAt: string;
};

// ニュースサイトからのスクレイピング
const scrapeNews = async (url: string): Promise<readonly NewsArticle[]> => {
  try {
    const response = await axios.get(url, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
      }
    });
    
    const $ = cheerio.load(response.data);
    const articles: NewsArticle[] = [];
    
    // 記事要素を選択
    $('.article-item').each((index, element) => {
      const title = $(element).find('.title').text().trim();
      const url = $(element).find('a').attr('href') || '';
      const summary = $(element).find('.summary').text().trim();
      const publishedAt = $(element).find('.date').text().trim();
      
      if (title && url) {
        articles.push({
          title,
          url: url.startsWith('http') ? url : `https://example.com${url}`,
          summary,
          publishedAt
        });
      }
    });
    
    return articles;
  } catch (error) {
    console.error('スクレイピングエラー:', error);
    return [];
  }
};

// 使用例
const newsArticles = await scrapeNews('https://example-news-site.com');
console.log('取得した記事数:', newsArticles.length);
```

### 2. Puppeteerを使った動的スクレイピング

Puppeteerは、Chrome/Chromiumを制御して動的コンテンツを取得できます。

```typescript
import puppeteer from 'puppeteer';

type Product = {
  readonly name: string;
  readonly price: number;
  readonly rating: number;
  readonly imageUrl: string;
  readonly availability: boolean;
};

// Eコマースサイトからの商品情報スクレイピング
const scrapeProducts = async (url: string): Promise<readonly Product[]> => {
  const browser = await puppeteer.launch({
    headless: true,
    args: ['--no-sandbox', '--disable-setuid-sandbox']
  });
  
  try {
    const page = await browser.newPage();
    
    // User-Agentを設定
    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36');
    
    // ページにアクセス
    await page.goto(url, { waitUntil: 'networkidle2' });
    
    // 動的コンテンツの読み込みを待機
    await page.waitForSelector('.product-item', { timeout: 10000 });
    
    // 商品情報を抽出
    const products = await page.evaluate(() => {
      const productElements = document.querySelectorAll('.product-item');
      const products: Product[] = [];
      
      productElements.forEach((element) => {
        const nameElement = element.querySelector('.product-name');
        const priceElement = element.querySelector('.product-price');
        const ratingElement = element.querySelector('.product-rating');
        const imageElement = element.querySelector('.product-image img');
        const availabilityElement = element.querySelector('.availability');
        
        if (nameElement && priceElement) {
          const name = nameElement.textContent?.trim() || '';
          const priceText = priceElement.textContent?.replace(/[^\d.]/g, '') || '0';
          const price = parseFloat(priceText);
          const rating = ratingElement ? parseFloat(ratingElement.textContent || '0') : 0;
          const imageUrl = (imageElement as HTMLImageElement)?.src || '';
          const availability = availabilityElement?.textContent?.includes('在庫あり') || false;
          
          products.push({
            name,
            price,
            rating,
            imageUrl,
            availability
          });
        }
      });
      
      return products;
    });
    
    return products;
  } finally {
    await browser.close();
  }
};

// ページネーション対応のスクレイピング
const scrapeAllProducts = async (baseUrl: string, maxPages: number): Promise<readonly Product[]> => {
  const allProducts: Product[] = [];
  
  for (let page = 1; page <= maxPages; page++) {
    const url = `${baseUrl}?page=${page}`;
    const products = await scrapeProducts(url);
    
    if (products.length === 0) break; // 商品がない場合は終了
    
    allProducts.push(...products);
    
    // レート制限を避けるため待機
    await new Promise(resolve => setTimeout(resolve, 2000));
  }
  
  return allProducts;
};
```

### 3. Playwrightを使った高度なスクレイピング

Playwrightは、複数のブラウザエンジンをサポートする現代的なスクレイピングライブラリです。

```typescript
import { chromium, type Browser, type Page } from 'playwright';

type SocialMediaPost = {
  readonly author: string;
  readonly content: string;
  readonly likes: number;
  readonly shares: number;
  readonly timestamp: string;
  readonly hashtags: readonly string[];
};

class SocialMediaScraper {
  private browser: Browser | null = null;
  private page: Page | null = null;
  
  async initialize(): Promise<void> {
    this.browser = await chromium.launch({
      headless: true
    });
    this.page = await this.browser.newPage();
    
    // モバイルエミュレーション
    await this.page.emulate({
      viewport: { width: 375, height: 667 },
      userAgent: 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15'
    });
  }
  
  async scrapePosts(hashtag: string, maxPosts: number): Promise<readonly SocialMediaPost[]> {
    if (!this.page) throw new Error('ブラウザが初期化されていません');
    
    const url = `https://example-social-site.com/hashtag/${hashtag}`;
    await this.page.goto(url);
    
    // 無限スクロールで投稿を読み込み
    let posts: SocialMediaPost[] = [];
    let previousHeight = 0;
    
    while (posts.length < maxPosts) {
      // 現在の投稿を取得
      const currentPosts = await this.page.evaluate(() => {
        const postElements = document.querySelectorAll('.post-item');
        const posts: SocialMediaPost[] = [];
        
        postElements.forEach((element) => {
          const author = element.querySelector('.author')?.textContent?.trim() || '';
          const content = element.querySelector('.content')?.textContent?.trim() || '';
          const likesText = element.querySelector('.likes')?.textContent?.replace(/[^\d]/g, '') || '0';
          const sharesText = element.querySelector('.shares')?.textContent?.replace(/[^\d]/g, '') || '0';
          const timestamp = element.querySelector('.timestamp')?.textContent?.trim() || '';
          const hashtagElements = element.querySelectorAll('.hashtag');
          const hashtags = Array.from(hashtagElements).map(el => el.textContent?.trim() || '');
          
          posts.push({
            author,
            content,
            likes: parseInt(likesText, 10),
            shares: parseInt(sharesText, 10),
            timestamp,
            hashtags: hashtags.filter(tag => tag.length > 0)
          });
        });
        
        return posts;
      });
      
      posts = currentPosts;
      
      // スクロールして新しい投稿を読み込み
      const currentHeight = await this.page.evaluate(() => document.body.scrollHeight);
      await this.page.evaluate(() => window.scrollTo(0, document.body.scrollHeight));
      
      // 新しいコンテンツが読み込まれるまで待機
      await this.page.waitForTimeout(2000);
      
      // スクロール位置が変わらない場合は終了
      if (currentHeight === previousHeight) break;
      previousHeight = currentHeight;
    }
    
    return posts.slice(0, maxPosts);
  }
  
  async close(): Promise<void> {
    if (this.browser) {
      await this.browser.close();
    }
  }
}

// 使用例
const scraper = new SocialMediaScraper();
await scraper.initialize();
const posts = await scraper.scrapePosts('typescript', 50);
await scraper.close();
```

## 高度なスクレイピングテクニック

### 1. プロキシローテーション

```typescript
type ProxyConfig = {
  readonly host: string;
  readonly port: number;
  readonly username?: string;
  readonly password?: string;
};

class ProxyRotator {
  private proxies: readonly ProxyConfig[];
  private currentIndex = 0;
  
  constructor(proxies: readonly ProxyConfig[]) {
    this.proxies = proxies;
  }
  
  getNextProxy(): ProxyConfig {
    const proxy = this.proxies[this.currentIndex];
    this.currentIndex = (this.currentIndex + 1) % this.proxies.length;
    return proxy;
  }
  
  async makeRequest(url: string): Promise<string> {
    const proxy = this.getNextProxy();
    
    const response = await axios.get(url, {
      proxy: {
        host: proxy.host,
        port: proxy.port,
        auth: proxy.username && proxy.password ? {
          username: proxy.username,
          password: proxy.password
        } : undefined
      },
      timeout: 10000
    });
    
    return response.data;
  }
}
```

### 2. レート制限とリトライ機能

```typescript
class RateLimitedScraper {
  private lastRequestTime = 0;
  private readonly minInterval: number;
  private readonly maxRetries: number;
  
  constructor(minIntervalMs: number = 1000, maxRetries: number = 3) {
    this.minInterval = minIntervalMs;
    this.maxRetries = maxRetries;
  }
  
  async scrapeWithRetry<T>(
    url: string,
    scraper: (html: string) => T
  ): Promise<T> {
    let lastError: Error | null = null;
    
    for (let attempt = 0; attempt < this.maxRetries; attempt++) {
      try {
        // レート制限を守る
        await this.waitForNextRequest();
        
        const response = await axios.get(url, {
          headers: {
            'User-Agent': this.getRandomUserAgent()
          }
        });
        
        return scraper(response.data);
      } catch (error) {
        lastError = error as Error;
        
        if (this.shouldRetry(error as Error)) {
          await this.waitBeforeRetry(attempt);
          continue;
        }
        
        throw error;
      }
    }
    
    throw lastError || new Error('最大リトライ回数に達しました');
  }
  
  private async waitForNextRequest(): Promise<void> {
    const now = Date.now();
    const timeSinceLastRequest = now - this.lastRequestTime;
    
    if (timeSinceLastRequest < this.minInterval) {
      await new Promise(resolve => 
        setTimeout(resolve, this.minInterval - timeSinceLastRequest)
      );
    }
    
    this.lastRequestTime = Date.now();
  }
  
  private getRandomUserAgent(): string {
    const userAgents = [
      'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
      'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
      'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
      'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15'
    ];
    
    return userAgents[Math.floor(Math.random() * userAgents.length)];
  }
  
  private shouldRetry(error: Error): boolean {
    const statusCode = (error as any)?.response?.status;
    return statusCode >= 500 || statusCode === 429; // サーバーエラーまたはレート制限
  }
  
  private async waitBeforeRetry(attempt: number): Promise<void> {
    const delay = Math.pow(2, attempt) * 1000; // 指数バックオフ
    await new Promise(resolve => setTimeout(resolve, delay));
  }
}
```

### 3. データの保存と管理

```typescript
import { writeFileSync, readFileSync, existsSync } from 'fs';
import { join } from 'path';

type ScrapedData = {
  readonly timestamp: string;
  readonly url: string;
  readonly data: unknown;
};

class DataManager {
  private readonly dataDir: string;
  
  constructor(dataDir: string = './scraped-data') {
    this.dataDir = dataDir;
  }
  
  saveData(filename: string, data: unknown, url: string): void {
    const scrapedData: ScrapedData = {
      timestamp: new Date().toISOString(),
      url,
      data
    };
    
    const filePath = join(this.dataDir, `${filename}.json`);
    writeFileSync(filePath, JSON.stringify(scrapedData, null, 2));
  }
  
  loadData(filename: string): ScrapedData | null {
    const filePath = join(this.dataDir, `${filename}.json`);
    
    if (!existsSync(filePath)) {
      return null;
    }
    
    const content = readFileSync(filePath, 'utf-8');
    return JSON.parse(content);
  }
  
  // 差分検出
  detectChanges(filename: string, newData: unknown): {
    readonly hasChanges: boolean;
    readonly changes: readonly string[];
  } {
    const oldData = this.loadData(filename);
    
    if (!oldData) {
      return { hasChanges: true, changes: ['新規データ'] };
    }
    
    const changes: string[] = [];
    const oldDataStr = JSON.stringify(oldData.data);
    const newDataStr = JSON.stringify(newData);
    
    if (oldDataStr !== newDataStr) {
      changes.push('データ内容が変更されました');
    }
    
    return {
      hasChanges: changes.length > 0,
      changes
    };
  }
}
```

## 法的・倫理的考慮事項

### 1. robots.txtの確認

```typescript
const checkRobotsTxt = async (baseUrl: string): Promise<{
  readonly allowed: boolean;
  readonly delay: number;
  readonly disallowedPaths: readonly string[];
}> => {
  try {
    const robotsUrl = new URL('/robots.txt', baseUrl).toString();
    const response = await axios.get(robotsUrl);
    const robotsContent = response.data;
    
    const lines = robotsContent.split('\n');
    const disallowedPaths: string[] = [];
    let delay = 0;
    
    for (const line of lines) {
      if (line.startsWith('Disallow:')) {
        disallowedPaths.push(line.substring(9).trim());
      } else if (line.startsWith('Crawl-delay:')) {
        delay = parseInt(line.substring(12).trim(), 10);
      }
    }
    
    return {
      allowed: true,
      delay,
      disallowedPaths
    };
  } catch (error) {
    console.warn('robots.txtが見つからないか、アクセスできません');
    return {
      allowed: true,
      delay: 1,
      disallowedPaths: []
    };
  }
};
```

### 2. 利用規約の確認

```typescript
const checkTermsOfService = (url: string): {
  readonly scrapingAllowed: boolean;
  readonly restrictions: readonly string[];
} => {
  // 一般的な制限事項のチェック
  const restrictions: string[] = [];
  
  // 商用利用の制限
  if (url.includes('example.com')) {
    restrictions.push('商用利用は禁止されています');
  }
  
  // データの再配布制限
  if (url.includes('news-site.com')) {
    restrictions.push('データの再配布は禁止されています');
  }
  
  return {
    scrapingAllowed: restrictions.length === 0,
    restrictions
  };
};
```

## パフォーマンス最適化

### 1. 並列処理

```typescript
const scrapeMultipleUrls = async (
  urls: readonly string[],
  concurrency: number = 3
): Promise<readonly unknown[]> => {
  const results: unknown[] = [];
  const chunks = [];
  
  // URLをチャンクに分割
  for (let i = 0; i < urls.length; i += concurrency) {
    chunks.push(urls.slice(i, i + concurrency));
  }
  
  for (const chunk of chunks) {
    const chunkPromises = chunk.map(async (url) => {
      try {
        const response = await axios.get(url);
        return { url, data: response.data, success: true };
      } catch (error) {
        return { url, error: error as Error, success: false };
      }
    });
    
    const chunkResults = await Promise.all(chunkPromises);
    results.push(...chunkResults);
    
    // チャンク間で待機
    await new Promise(resolve => setTimeout(resolve, 1000));
  }
  
  return results;
};
```

### 2. メモリ効率の良い処理

```typescript
const streamScrape = async (
  urls: readonly string[],
  processor: (data: unknown) => void
): Promise<void> => {
  for (const url of urls) {
    try {
      const response = await axios.get(url);
      processor(response.data);
      
      // メモリを解放
      if (global.gc) {
        global.gc();
      }
    } catch (error) {
      console.error(`URL ${url} の処理中にエラーが発生:`, error);
    }
  }
};
```

## まとめ

この記事では、TypeScriptを使ったWebスクレイピングの包括的な実装方法を紹介しました：

- **基本的なスクレイピング**: Cheerio、Puppeteer、Playwrightの活用
- **高度なテクニック**: プロキシローテーション、レート制限、リトライ機能
- **データ管理**: 保存、差分検出、効率的な処理
- **法的・倫理的考慮**: robots.txt、利用規約の確認
- **パフォーマンス最適化**: 並列処理、メモリ効率

スクレイピングを行う際は、必ず以下の点に注意してください：

1. **robots.txtの確認**: サイトの利用規約を尊重
2. **レート制限の遵守**: サーバーに負荷をかけない
3. **適切なUser-Agent**: 正体を明かす
4. **エラーハンドリング**: 堅牢な実装
5. **データの適切な使用**: 法的・倫理的な配慮

これらの原則を守ることで、責任あるスクレイピングが可能になります。
