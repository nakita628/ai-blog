---
date: 2025-08-31
title: TypeScriptで学ぶWebスクレイピング：実装からベストプラクティスまで完全ガイド
description: TypeScriptを使ったWebスクレイピングの実装方法を解説。Puppeteer、Cheerio、Playwrightを使った実践的な例と、法的・倫理的考慮事項、パフォーマンス最適化まで網羅。
tags:
  - typescript
  - web-scraping
  - puppeteer
  - cheerio
  - playwright
  - automation
  - data-extraction
  - web-development
prev:
  text: 'TypeScriptで学ぶ現実世界のアルゴリズム：交通、経済、自然現象の数値モデル化'
  link: '/posts/2025/08/30'
next:
  text: 'Tailwind CSS完全ガイド：初心者向けベストプラクティスと実践テクニック'
  link: '/posts/2025/09/01'
---

# TypeScriptで学ぶWebスクレイピング：実装からベストプラクティスまで完全ガイド

Webスクレイピングは、Webサイトからデータを自動的に収集する技術です。この記事では、TypeScriptを使ったスクレイピングの実装方法、ベストプラクティス、法的・倫理的考慮事項まで包括的に解説します。

## スクレイピングとは何か

Webスクレイピングは、Webサイトから構造化されたデータを自動的に抽出するプロセスです。価格比較、ニュース収集、市場調査、SEO分析など、様々な用途で活用されています。

### スクレイピングの種類

- **静的スクレイピング**: HTMLから直接データを抽出
- **動的スクレイピング**: JavaScriptで生成されるコンテンツを取得
- **APIスクレイピング**: 公開APIからデータを取得
- **ヘッドレスブラウザ**: ブラウザエンジンを使用したスクレイピング

## 基本的なスクレイピング実装

### 1. Cheerioを使った静的スクレイピング

Cheerioは、サーバーサイドでjQueryライクなAPIを提供するライブラリです。

```typescript
import * as cheerio from 'cheerio'
import axios from 'axios'

type NewsArticle = {
  readonly title: string
  readonly url: string
  readonly summary: string
  readonly publishedAt: string
}

// ニュースサイトからのスクレイピング
const scrapeNews = async (url: string): Promise<readonly NewsArticle[]> => {
  try {
    const response = await axios.get(url, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
      },
    })

    const $ = cheerio.load(response.data)
    const articles: NewsArticle[] = []

    // 記事要素を選択
    $('.article-item').each((index, element) => {
      const title = $(element).find('.title').text().trim()
      const url = $(element).find('a').attr('href') || ''
      const summary = $(element).find('.summary').text().trim()
      const publishedAt = $(element).find('.date').text().trim()

      if (title && url) {
        articles.push({
          title,
          url: url.startsWith('http') ? url : `https://example.com${url}`,
          summary,
          publishedAt,
        })
      }
    })

    return articles
  } catch (error) {
    console.error('スクレイピングエラー:', error)
    return []
  }
}

// 使用例
const newsArticles = await scrapeNews('https://example-news-site.com')
console.log('取得した記事数:', newsArticles.length)
```

### 2. Puppeteerを使った動的スクレイピング

Puppeteerは、Chrome/Chromiumを制御して動的コンテンツを取得できます。

```typescript
import puppeteer from 'puppeteer'

type Product = {
  readonly name: string
  readonly price: number
  readonly rating: number
  readonly imageUrl: string
  readonly availability: boolean
}

// Eコマースサイトからの商品情報スクレイピング
const scrapeProducts = async (url: string): Promise<readonly Product[]> => {
  const browser = await puppeteer.launch({
    headless: true,
    args: ['--no-sandbox', '--disable-setuid-sandbox'],
  })

  try {
    const page = await browser.newPage()

    // User-Agentを設定
    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

    // ページにアクセス
    await page.goto(url, { waitUntil: 'networkidle2' })

    // 動的コンテンツの読み込みを待機
    await page.waitForSelector('.product-item', { timeout: 10000 })

    // 商品情報を抽出
    const products = await page.evaluate(() => {
      const productElements = document.querySelectorAll('.product-item')
      const products: Product[] = []

      productElements.forEach((element) => {
        const nameElement = element.querySelector('.product-name')
        const priceElement = element.querySelector('.product-price')
        const ratingElement = element.querySelector('.product-rating')
        const imageElement = element.querySelector('.product-image img')
        const availabilityElement = element.querySelector('.availability')

        if (nameElement && priceElement) {
          const name = nameElement.textContent?.trim() || ''
          const priceText = priceElement.textContent?.replace(/[^\d.]/g, '') || '0'
          const price = parseFloat(priceText)
          const rating = ratingElement ? parseFloat(ratingElement.textContent || '0') : 0
          const imageUrl = (imageElement as HTMLImageElement)?.src || ''
          const availability = availabilityElement?.textContent?.includes('在庫あり') || false

          products.push({
            name,
            price,
            rating,
            imageUrl,
            availability,
          })
        }
      })

      return products
    })

    return products
  } finally {
    await browser.close()
  }
}

// ページネーション対応のスクレイピング
const scrapeAllProducts = async (
  baseUrl: string,
  maxPages: number,
): Promise<readonly Product[]> => {
  const allProducts: Product[] = []

  for (let page = 1; page <= maxPages; page++) {
    const url = `${baseUrl}?page=${page}`
    const products = await scrapeProducts(url)

    if (products.length === 0) break // 商品がない場合は終了

    allProducts.push(...products)

    // レート制限を避けるため待機
    await new Promise((resolve) => setTimeout(resolve, 2000))
  }

  return allProducts
}
```

### 3. Playwrightを使った高度なスクレイピング

Playwrightは、複数のブラウザエンジンをサポートする現代的なスクレイピングライブラリです。

```typescript
import { chromium, type Browser, type Page } from 'playwright'

type SocialMediaPost = {
  readonly author: string
  readonly content: string
  readonly likes: number
  readonly shares: number
  readonly timestamp: string
  readonly hashtags: readonly string[]
}

class SocialMediaScraper {
  private browser: Browser | null = null
  private page: Page | null = null

  async initialize(): Promise<void> {
    this.browser = await chromium.launch({
      headless: true,
    })
    this.page = await this.browser.newPage()

    // モバイルエミュレーション
    await this.page.emulate({
      viewport: { width: 375, height: 667 },
      userAgent: 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15',
    })
  }

  async scrapePosts(hashtag: string, maxPosts: number): Promise<readonly SocialMediaPost[]> {
    if (!this.page) throw new Error('ブラウザが初期化されていません')

    const url = `https://example-social-site.com/hashtag/${hashtag}`
    await this.page.goto(url)

    // 無限スクロールで投稿を読み込み
    let posts: SocialMediaPost[] = []
    let previousHeight = 0

    while (posts.length < maxPosts) {
      // 現在の投稿を取得
      const currentPosts = await this.page.evaluate(() => {
        const postElements = document.querySelectorAll('.post-item')
        const posts: SocialMediaPost[] = []

        postElements.forEach((element) => {
          const author = element.querySelector('.author')?.textContent?.trim() || ''
          const content = element.querySelector('.content')?.textContent?.trim() || ''
          const likesText =
            element.querySelector('.likes')?.textContent?.replace(/[^\d]/g, '') || '0'
          const sharesText =
            element.querySelector('.shares')?.textContent?.replace(/[^\d]/g, '') || '0'
          const timestamp = element.querySelector('.timestamp')?.textContent?.trim() || ''
          const hashtagElements = element.querySelectorAll('.hashtag')
          const hashtags = Array.from(hashtagElements).map((el) => el.textContent?.trim() || '')

          posts.push({
            author,
            content,
            likes: parseInt(likesText, 10),
            shares: parseInt(sharesText, 10),
            timestamp,
            hashtags: hashtags.filter((tag) => tag.length > 0),
          })
        })

        return posts
      })

      posts = currentPosts

      // スクロールして新しい投稿を読み込み
      const currentHeight = await this.page.evaluate(() => document.body.scrollHeight)
      await this.page.evaluate(() => window.scrollTo(0, document.body.scrollHeight))

      // 新しいコンテンツが読み込まれるまで待機
      await this.page.waitForTimeout(2000)

      // スクロール位置が変わらない場合は終了
      if (currentHeight === previousHeight) break
      previousHeight = currentHeight
    }

    return posts.slice(0, maxPosts)
  }

  async close(): Promise<void> {
    if (this.browser) {
      await this.browser.close()
    }
  }
}

// 使用例
const scraper = new SocialMediaScraper()
await scraper.initialize()
const posts = await scraper.scrapePosts('typescript', 50)
await scraper.close()
```

## 高度なスクレイピングテクニック

### 1. プロキシローテーション

```typescript
type ProxyConfig = {
  readonly host: string
  readonly port: number
  readonly username?: string
  readonly password?: string
}

class ProxyRotator {
  private proxies: readonly ProxyConfig[]
  private currentIndex = 0

  constructor(proxies: readonly ProxyConfig[]) {
    this.proxies = proxies
  }

  getNextProxy(): ProxyConfig {
    const proxy = this.proxies[this.currentIndex]
    this.currentIndex = (this.currentIndex + 1) % this.proxies.length
    return proxy
  }

  async makeRequest(url: string): Promise<string> {
    const proxy = this.getNextProxy()

    const response = await axios.get(url, {
      proxy: {
        host: proxy.host,
        port: proxy.port,
        auth:
          proxy.username && proxy.password
            ? {
                username: proxy.username,
                password: proxy.password,
              }
            : undefined,
      },
      timeout: 10000,
    })

    return response.data
  }
}
```

### 2. レート制限とリトライ機能

```typescript
class RateLimitedScraper {
  private lastRequestTime = 0
  private readonly minInterval: number
  private readonly maxRetries: number

  constructor(minIntervalMs: number = 1000, maxRetries: number = 3) {
    this.minInterval = minIntervalMs
    this.maxRetries = maxRetries
  }

  async scrapeWithRetry<T>(url: string, scraper: (html: string) => T): Promise<T> {
    let lastError: Error | null = null

    for (let attempt = 0; attempt < this.maxRetries; attempt++) {
      try {
        // レート制限を守る
        await this.waitForNextRequest()

        const response = await axios.get(url, {
          headers: {
            'User-Agent': this.getRandomUserAgent(),
          },
        })

        return scraper(response.data)
      } catch (error) {
        lastError = error as Error

        if (this.shouldRetry(error as Error)) {
          await this.waitBeforeRetry(attempt)
          continue
        }

        throw error
      }
    }

    throw lastError || new Error('最大リトライ回数に達しました')
  }

  private async waitForNextRequest(): Promise<void> {
    const now = Date.now()
    const timeSinceLastRequest = now - this.lastRequestTime

    if (timeSinceLastRequest < this.minInterval) {
      await new Promise((resolve) => setTimeout(resolve, this.minInterval - timeSinceLastRequest))
    }

    this.lastRequestTime = Date.now()
  }

  private getRandomUserAgent(): string {
    const userAgents = [
      'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
      'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
      'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
      'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15',
    ]

    return userAgents[Math.floor(Math.random() * userAgents.length)]
  }

  private shouldRetry(error: Error): boolean {
    const statusCode = (error as any)?.response?.status
    return statusCode >= 500 || statusCode === 429 // サーバーエラーまたはレート制限
  }

  private async waitBeforeRetry(attempt: number): Promise<void> {
    const delay = Math.pow(2, attempt) * 1000 // 指数バックオフ
    await new Promise((resolve) => setTimeout(resolve, delay))
  }
}
```

### 3. データの保存と管理

```typescript
import { writeFileSync, readFileSync, existsSync } from 'fs'
import { join } from 'path'

type ScrapedData = {
  readonly timestamp: string
  readonly url: string
  readonly data: unknown
}

class DataManager {
  private readonly dataDir: string

  constructor(dataDir: string = './scraped-data') {
    this.dataDir = dataDir
  }

  saveData(filename: string, data: unknown, url: string): void {
    const scrapedData: ScrapedData = {
      timestamp: new Date().toISOString(),
      url,
      data,
    }

    const filePath = join(this.dataDir, `${filename}.json`)
    writeFileSync(filePath, JSON.stringify(scrapedData, null, 2))
  }

  loadData(filename: string): ScrapedData | null {
    const filePath = join(this.dataDir, `${filename}.json`)

    if (!existsSync(filePath)) {
      return null
    }

    const content = readFileSync(filePath, 'utf-8')
    return JSON.parse(content)
  }

  // 差分検出
  detectChanges(
    filename: string,
    newData: unknown,
  ): {
    readonly hasChanges: boolean
    readonly changes: readonly string[]
  } {
    const oldData = this.loadData(filename)

    if (!oldData) {
      return { hasChanges: true, changes: ['新規データ'] }
    }

    const changes: string[] = []
    const oldDataStr = JSON.stringify(oldData.data)
    const newDataStr = JSON.stringify(newData)

    if (oldDataStr !== newDataStr) {
      changes.push('データ内容が変更されました')
    }

    return {
      hasChanges: changes.length > 0,
      changes,
    }
  }
}
```

## 法的・倫理的考慮事項

### 1. robots.txtの確認

```typescript
const checkRobotsTxt = async (
  baseUrl: string,
): Promise<{
  readonly allowed: boolean
  readonly delay: number
  readonly disallowedPaths: readonly string[]
}> => {
  try {
    const robotsUrl = new URL('/robots.txt', baseUrl).toString()
    const response = await axios.get(robotsUrl)
    const robotsContent = response.data

    const lines = robotsContent.split('\n')
    const disallowedPaths: string[] = []
    let delay = 0

    for (const line of lines) {
      if (line.startsWith('Disallow:')) {
        disallowedPaths.push(line.substring(9).trim())
      } else if (line.startsWith('Crawl-delay:')) {
        delay = parseInt(line.substring(12).trim(), 10)
      }
    }

    return {
      allowed: true,
      delay,
      disallowedPaths,
    }
  } catch (error) {
    console.warn('robots.txtが見つからないか、アクセスできません')
    return {
      allowed: true,
      delay: 1,
      disallowedPaths: [],
    }
  }
}
```

### 2. 利用規約の確認

```typescript
const checkTermsOfService = (
  url: string,
): {
  readonly scrapingAllowed: boolean
  readonly restrictions: readonly string[]
} => {
  // 一般的な制限事項のチェック
  const restrictions: string[] = []

  // 商用利用の制限
  if (url.includes('example.com')) {
    restrictions.push('商用利用は禁止されています')
  }

  // データの再配布制限
  if (url.includes('news-site.com')) {
    restrictions.push('データの再配布は禁止されています')
  }

  return {
    scrapingAllowed: restrictions.length === 0,
    restrictions,
  }
}
```

## パフォーマンス最適化

### 1. 並列処理

```typescript
const scrapeMultipleUrls = async (
  urls: readonly string[],
  concurrency: number = 3,
): Promise<readonly unknown[]> => {
  const results: unknown[] = []
  const chunks = []

  // URLをチャンクに分割
  for (let i = 0; i < urls.length; i += concurrency) {
    chunks.push(urls.slice(i, i + concurrency))
  }

  for (const chunk of chunks) {
    const chunkPromises = chunk.map(async (url) => {
      try {
        const response = await axios.get(url)
        return { url, data: response.data, success: true }
      } catch (error) {
        return { url, error: error as Error, success: false }
      }
    })

    const chunkResults = await Promise.all(chunkPromises)
    results.push(...chunkResults)

    // チャンク間で待機
    await new Promise((resolve) => setTimeout(resolve, 1000))
  }

  return results
}
```

### 2. メモリ効率の良い処理

```typescript
const streamScrape = async (
  urls: readonly string[],
  processor: (data: unknown) => void,
): Promise<void> => {
  for (const url of urls) {
    try {
      const response = await axios.get(url)
      processor(response.data)

      // メモリを解放
      if (global.gc) {
        global.gc()
      }
    } catch (error) {
      console.error(`URL ${url} の処理中にエラーが発生:`, error)
    }
  }
}
```

## まとめ

この記事では、TypeScriptを使ったWebスクレイピングの包括的な実装方法を紹介しました：

- **基本的なスクレイピング**: Cheerio、Puppeteer、Playwrightの活用
- **高度なテクニック**: プロキシローテーション、レート制限、リトライ機能
- **データ管理**: 保存、差分検出、効率的な処理
- **法的・倫理的考慮**: robots.txt、利用規約の確認
- **パフォーマンス最適化**: 並列処理、メモリ効率

スクレイピングを行う際は、必ず以下の点に注意してください：

1. **robots.txtの確認**: サイトの利用規約を尊重
2. **レート制限の遵守**: サーバーに負荷をかけない
3. **適切なUser-Agent**: 正体を明かす
4. **エラーハンドリング**: 堅牢な実装
5. **データの適切な使用**: 法的・倫理的な配慮

これらの原則を守ることで、責任あるスクレイピングが可能になります。
